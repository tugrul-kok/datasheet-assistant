import os
import sys
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings

# Import path'i dÃ¼zelt - proje kÃ¶k dizinini ekle
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.rag import get_rag_chain

# 1. JÃ¼ri Modellerini Ayarla (Mistral KullanacaÄŸÄ±z)
# DeÄŸerlendirmeyi yapan modelin biraz zeki olmasÄ± gerekir, Mistral Small/Large iyidir.
judge_llm = LangchainLLMWrapper(ChatMistralAI(model="mistral-small", temperature=0))
judge_embeddings = LangchainEmbeddingsWrapper(MistralAIEmbeddings())

# Metriklere jÃ¼riyi tanÄ±t
faithfulness.llm = judge_llm
answer_relevancy.llm = judge_llm
answer_relevancy.embeddings = judge_embeddings

def run_evaluation():
    print("ğŸš€ DeÄŸerlendirme baÅŸlÄ±yor...")
    
    # 2. Test Verisi (Bunu kendi PDF iÃ§eriÄŸine gÃ¶re MUTLAKA deÄŸiÅŸtir)
    # GerÃ§ek hayatta bu "Golden Dataset" olarak dÄ±ÅŸarÄ±dan yÃ¼klenir.
    test_questions = [
        "TuÄŸrul KÃ¶k hangi Ã¼niversiteden mezun olmuÅŸtur?",
        "TuÄŸrul'un uzmanlÄ±k alanlarÄ± nelerdir?",
        "Madlen ÅŸirketinde hangi projeyi geliÅŸtirmiÅŸtir?" 
    ]
    
    # RAG Zincirini YÃ¼kle
    chain = get_rag_chain()
    
    results = {
        "question": [],
        "answer": [],
        "contexts": [],  # Ragas "contexts" (liste) bekler
    }

    # 3. SorularÄ± Chatbot'a Sor ve CevaplarÄ± Topla
    print("ğŸ¤– Sorular chatbot'a soruluyor...")
    for q in test_questions:
        response = chain(q)
        
        results["question"].append(q)
        results["answer"].append(response["answer"])
        
        # Context'leri string listesi haline getir
        context_list = [doc.page_content for doc in response["source_documents"]]
        results["contexts"].append(context_list)

    # 4. Veriyi Dataset FormatÄ±na Ã‡evir
    dataset = Dataset.from_dict(results)

    # 5. Ragas ile Puanla
    print("âš–ï¸  Ragas puanlamasÄ± yapÄ±lÄ±yor (Bu biraz sÃ¼rebilir)...")
    scores = evaluate(
        dataset=dataset,
        metrics=[faithfulness, answer_relevancy],
        llm=judge_llm, 
        embeddings=judge_embeddings
    )

    # 6. SonuÃ§larÄ± GÃ¶ster ve Kaydet
    df = scores.to_pandas()
    print("\nğŸ“Š DeÄŸerlendirme SonuÃ§larÄ±:")
    print(df[["user_input", "faithfulness", "answer_relevancy"]])
    
    # Ortalama skoru yazdÄ±r
    print("\nğŸ“ˆ Ortalama Skorlar:")
    print(scores)
    
    # Ä°stersen CSV olarak kaydet (MLOps'ta bu dosya versiyonlanÄ±r)
    df.to_csv("evaluation_results.csv", index=False)

if __name__ == "__main__":
    run_evaluation()