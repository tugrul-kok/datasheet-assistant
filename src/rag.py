import os
from dotenv import load_dotenv
from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

DB_PATH = "chroma_db"

# Global Modeller
embedding_function = MistralAIEmbeddings()
vector_db = Chroma(persist_directory=DB_PATH, embedding_function=embedding_function)
# Router iÃ§in sÄ±caklÄ±k 0 olsun, karar net olsun
llm = ChatMistralAI(model="mistral-small", temperature=0.0)

# --- 1. ROUTER (KARAR MEKANÄ°ZMASI) ---
def semantic_router(query):
    """
    Sorunun hangi dokÃ¼manla ilgili olduÄŸunu anlayan ajan.
    """
    print(f"ğŸ¤” Router DÃ¼ÅŸÃ¼nÃ¼yor: '{query}'")
    
    router_template = """
    You are an expert intent classifier.
    Classify the user question into one of the following document keys:
    
    - stm32f4.pdf (Keywords: F4, F407, Discovery, 168MHz, DSP, Cortex-M4)
    - stm32f1.pdf (Keywords: F1, F103, Blue Pill, 72MHz, Cortex-M3)
    - bg96.pdf (Keywords: Modem, LTE, Cellular, NB-IoT, Cat M1, GNSS, Quectel)
    - stm32u5.pdf (Keywords: U5, Low Power, Cortex-M33)
    - auto (If the question is general or ambiguous)

    Examples:
    Q: What is the clock speed of F4? -> stm32f4.pdf
    Q: Does the modem support GPS? -> bg96.pdf
    Q: What is an interrupt? -> auto

    Question: {question}
    
    Return ONLY the filename (or 'auto'). Do not explain.
    """
    
    prompt = ChatPromptTemplate.from_template(router_template)
    chain = prompt | llm | StrOutputParser()
    
    try:
        route = chain.invoke({"question": query}).strip()
        print(f"ğŸ‘‰ Router KararÄ±: {route}")
        # Bazen model "Filename: stm32f4.pdf" diyebilir, temizleyelim
        for key in ["stm32f4.pdf", "stm32f1.pdf", "bg96.pdf", "stm32u5.pdf"]:
            if key in route:
                return key
        return "auto"
    except Exception as e:
        print(f"Router Error: {e}")
        return "auto"

# --- 2. RAG CHAIN (DÄ°NAMÄ°K) ---
def get_rag_chain(doc_filter=None):
    # Ayarlar
    search_kwargs = {"k": 6}
    
    # EÄŸer filtre varsa onu uygula
    if doc_filter and doc_filter != "auto":
        source_path = f"data/{doc_filter}"
        search_kwargs["filter"] = {"source": source_path}
        print(f"ğŸ” Filtering Context: Only using {source_path}")
    else:
        print("ğŸ” Context: Global Search (No Filter)")

    retriever = vector_db.as_retriever(search_kwargs=search_kwargs)
    
    # Prompt - Biraz daha konuÅŸkan hale getirdik (Relevancy iÃ§in)
    template = """
    You are a Senior Embedded Systems Engineer. 
    Answer the question based ONLY on the provided context.
    
    Rules:
    1. Start directly with the answer. 
    2. If the info is in a table, mention it (e.g., "According to Table 4...").
    3. If the context is empty or irrelevant, say "I cannot find this specific information in the selected document."
    4. Be concise but complete.

    Context:
    {context}
    
    Question: {input}
    
    Answer:
    """
    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
        {"context": retriever | format_docs, "input": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain, retriever

# --- 3. ANA FONKSÄ°YON ---
def ask_question(query, doc_filter="auto"):
    final_filter = doc_filter
    
    # EÄŸer kullanÄ±cÄ± "Auto" seÃ§tiyse, Router devreye girsin
    if doc_filter == "auto":
        predicted_filter = semantic_router(query)
        # Router "auto" demezse, onun tahminini kullanalÄ±m
        if predicted_filter != "auto":
            final_filter = predicted_filter
    
    # RAG Zincirini Ã§aÄŸÄ±r
    chain, retriever = get_rag_chain(final_filter)
    
    answer = chain.invoke(query)
    source_docs = retriever.invoke(query)
    
    return {
        "answer": answer,
        "source_documents": source_docs,
        "routed_to": final_filter # Debug iÃ§in bunu da gÃ¶relim
    }